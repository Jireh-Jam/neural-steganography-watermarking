{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba694e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6853ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Steganography: Image Key Embedding & Extraction\n",
    "\n",
    "# 1. Install and Import Required Libraries\n",
    "!pip install pillow matplotlib \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f15284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Utility Functions: Image I/O and Message Conversion\n",
    "def load_image(path, target_size=(128, 128)):\n",
    "    img = Image.open(path).convert('RGB').resize(target_size)\n",
    "    img_arr = np.asarray(img) / 255.0\n",
    "    return img_arr\n",
    "\n",
    "def save_image(img_arr, path):\n",
    "    img_uint8 = (img_arr * 255).astype(np.uint8)\n",
    "    Image.fromarray(img_uint8).save(path)\n",
    "\n",
    "def text_to_bits(text, length=128*128):\n",
    "    bits = ''.join(format(ord(c), '08b') for c in text)\n",
    "    bits = bits.ljust(length, '0')[:length]\n",
    "    bit_arr = np.array(list(bits), dtype=np.float32)\n",
    "    return bit_arr.reshape((1, 128, 128, 1))\n",
    "\n",
    "def bits_to_text(bit_array):\n",
    "    bits = ''.join(str(int(b > 0.5)) for b in bit_array.flatten())\n",
    "    chars = [chr(int(bits[i:i+8], 2)) for i in range(0, len(bits), 8)]\n",
    "    text = ''.join(chars)\n",
    "    return text.strip('\\x00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8816539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Neural Network Architectures\n",
    "\n",
    "def build_encoder(input_shape=(128, 128, 3), message_shape=(128, 128, 1)):\n",
    "    cover_input = layers.Input(shape=input_shape)\n",
    "    message_input = layers.Input(shape=message_shape)\n",
    "    x = layers.Concatenate()([cover_input, message_input])\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "    stego_output = layers.Conv2D(3, (1,1), activation='sigmoid', padding='same')(x)\n",
    "    return keras.Model([cover_input, message_input], stego_output, name='encoder')\n",
    "\n",
    "def build_decoder(stego_shape=(128, 128, 3)):\n",
    "    stego_input = layers.Input(shape=stego_shape)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(stego_input)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(x)\n",
    "    message_output = layers.Conv2D(1, (1,1), activation='sigmoid', padding='same')(x)\n",
    "    return keras.Model(stego_input, message_output, name='decoder')\n",
    "\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d83038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name, base_path=None):\n",
    "    if base_path is None:\n",
    "        base_path = os.getcwd()\n",
    "    full_path = os.path.join(base_path, folder_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    return full_path\n",
    "\n",
    "def load_all_imgs(path, size=(128,128)):\n",
    "    imgs = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff')):\n",
    "            full_path = os.path.join(path, filename)\n",
    "            img = Image.open(full_path).convert('RGB').resize(size)\n",
    "            imgs.append(np.asarray(img)/255.0)\n",
    "    return np.stack(imgs)\n",
    "\n",
    "# 3) Demo\n",
    "train_dir = r'C:/Users/User/Desktop/stable_signature/data/train/train'\n",
    "secret_message  = \"InvisibleKey!\"\n",
    "message_arr     = text_to_bits(secret_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ceb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # â€”â€“ Load and split your data â€”â€“\n",
    "# # (Fill in with your actual directories or data-loading logic)\n",
    "train_covers = load_all_imgs(train_dir)         # shape: (N_train, H, W, C)\n",
    "# train_bits   = np.stack([text_to_bits(msg) for msg in train_messages])  # shape: (N_train, msg_len)\n",
    "# val_covers   = load_all_imgs(val_dir)\n",
    "# val_bits     = np.stack([text_to_bits(msg) for msg in val_messages])\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Precompute once:\n",
    "message_arr = text_to_bits(secret_message)                   # numpy array, e.g. shape (msg_len,)\n",
    "message_tensor = tf.constant(message_arr, dtype=tf.float32)  # now a TF tensor\n",
    "\n",
    "def make_dataset(image_dir, batch_size, shuffle=True):\n",
    "    # 1) build list of fileâ€paths\n",
    "    files = [os.path.join(image_dir, f)\n",
    "             for f in os.listdir(image_dir)\n",
    "             if f.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff'))]\n",
    "\n",
    "    # 2) Create a dataset of fileâ€paths\n",
    "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(files))\n",
    "\n",
    "    # 3) Map each path â†’ (image, same message_tensor)\n",
    "    ds = ds.map(\n",
    "        lambda p: (parse_image(p), message_tensor),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # 4) Batch & prefetch\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# â€”â€” Hyper-params & checkpoints â€”â€”\n",
    "batch_size     = 16\n",
    "epochs         = 50\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# â€”â€” Precompute your secret message tensor â€”â€”\n",
    "message_arr    = text_to_bits(secret_message)                # numpy array of bits\n",
    "message_tensor = tf.constant(message_arr, dtype=tf.float32)  # scalar-sized TF tensor\n",
    "\n",
    "# â€”â€” Image loader from path â†’ normalized tensor â€”â€”\n",
    "def parse_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, [128, 128])\n",
    "    return img / 255.0\n",
    "\n",
    "# â€”â€” Build a streaming Dataset that zips each image with the same message_tensor â€”â€”\n",
    "def make_streaming_dataset(image_dir, batch_size, shuffle=True):\n",
    "    # 1) Grab file-patterns (png/jpg/etc). This does NOT load images yet.\n",
    "    files_ds = tf.data.Dataset.list_files(os.path.join(image_dir, \"*.*\"), shuffle=shuffle)\n",
    "    # 2) Map pathâ†’(image_tensor, message_tensor)\n",
    "    ds = files_ds.map(\n",
    "        lambda p: (parse_image(p), message_tensor),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    # 3) Batch & prefetch\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# â€”â€” Instantiate train & val streams â€”â€”\n",
    "train_ds = make_streaming_dataset(train_dir, batch_size, shuffle=True)\n",
    "val_ds   = make_streaming_dataset(val_dir,   batch_size, shuffle=False)\n",
    "\n",
    "# â€”â€” Checkpoint callback (weights only) â€”â€”\n",
    "ckpt_path = os.path.join(\n",
    "    checkpoint_dir,\n",
    "    \"stego_epoch{epoch:02d}_val{val_loss:.4f}.weights.h5\"\n",
    ")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# â€”â€” Train! (this will load only each batch into memory) â€”â€”\n",
    "history = encoder.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# â€”â€” Save final weights too â€”â€”\n",
    "encoder.save_weights(os.path.join(checkpoint_dir, \"stego_final.weights.h5\"))\n",
    "print(\"âœ… Done training; checkpoints in\", checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# â€”â€”â€” Precompute your message tensor â€”â€”â€”\n",
    "message_arr    = text_to_bits(secret_message)                # numpy array shape (msg_len,)\n",
    "message_tensor = tf.constant(message_arr, dtype=tf.float32)  # reuse for every example\n",
    "\n",
    "# â€”â€”â€” Image parsing helper â€”â€”â€”\n",
    "def parse_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, [128, 128])\n",
    "    return img / 255.0\n",
    "\n",
    "# â€”â€”â€” Dataset builder â€”â€”â€”\n",
    "def make_dataset(image_dir, batch_size, shuffle=True):\n",
    "    # 1) List image fileâ€paths\n",
    "    files = [\n",
    "        os.path.join(image_dir, f)\n",
    "        for f in os.listdir(image_dir)\n",
    "        if f.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff'))\n",
    "    ]\n",
    "    # 2) Create a Dataset of paths\n",
    "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(files))\n",
    "    # 3) Map each path â†’ (image, constant message)\n",
    "    ds = ds.map(\n",
    "        lambda p: (parse_image(p), message_tensor),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    # 4) Batch & prefetch\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# â€”â€”â€” Config & checkpoint callback â€”â€”â€”\n",
    "batch_size     = 16\n",
    "epochs         = 50\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "ckpt_path = os.path.join(\n",
    "    checkpoint_dir,\n",
    "    \"stego_epoch{epoch:02d}_val{val_loss:.4f}.weights.h5\"\n",
    ")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# â€”â€”â€” Build & train â€”â€”â€”\n",
    "train_ds = make_dataset(train_dir, batch_size, shuffle=True)\n",
    "val_ds   = make_dataset(val_dir,   batch_size, shuffle=False)\n",
    "\n",
    "history = encoder.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# â€”â€”â€” Save final weights â€”â€”â€”\n",
    "encoder.save_weights(\n",
    "    os.path.join(checkpoint_dir, \"stego_final.weights.h5\")\n",
    ")\n",
    "print(\"âœ… Training complete; checkpoints in\", checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# â€”â€” Config â€”â€” \n",
    "batch_size     = 16\n",
    "epochs         = 50\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# â€”â€” Helper: parse & preprocess one image path â†’ tensor â€”â€” \n",
    "def parse_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, [128, 128])\n",
    "    return tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "# â€”â€” Build a tf.data.Dataset from a folder + list of messages â€”â€” \n",
    "def make_dataset(image_dir, messages, batch_size, shuffle=True):\n",
    "    # 1) get filepaths\n",
    "    files = [os.path.join(image_dir, f) \n",
    "             for f in os.listdir(image_dir)\n",
    "             if f.lower().endswith(('.png','.jpg','jpeg','.bmp','.tiff'))]\n",
    "    # 2) convert messages â†’ bit-arrays\n",
    "    bits = np.stack([text_to_bits(msg) for msg in messages])\n",
    "    # 3) dataset of (path, bits)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((files, bits))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(files))\n",
    "    # 4) load image, pair with bits\n",
    "    ds = ds.map(lambda p, b: (parse_image(p), b),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # 5) batch & prefetch\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# â€”â€” Instantiate train & val datasets â€”â€” \n",
    "train_ds = make_dataset(train_dir, train_messages, batch_size, shuffle=True)\n",
    "val_ds   = make_dataset(val_dir,   val_messages,   batch_size, shuffle=False)\n",
    "\n",
    "# â€”â€” Checkpoint callback (.weights.h5 suffix!) â€”â€” \n",
    "ckpt_path = os.path.join(checkpoint_dir, \"stego_epoch{epoch:02d}_val{val_loss:.4f}.weights.h5\")\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=ckpt_path,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# â€”â€” Train streaming from disk â€”â€” \n",
    "history = encoder.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# â€”â€” Save final weights too â€”â€” \n",
    "encoder.save_weights(os.path.join(checkpoint_dir, \"stego_final.weights.h5\"))\n",
    "print(\"âœ… Training complete; checkpoints in\", checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = r\"C:/Users/User/Desktop/Watermarking_Project/stable_signature_2_0/test_images/\"      # where your test images are\n",
    "secret_message  = \"InvisibleKey!\"\n",
    "message_arr     = text_to_bits(secret_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Pick the latest .h5 in your checkpoints folder\n",
    "ckpt_files = glob.glob(os.path.join(checkpoint_dir, \"*.h5\"))\n",
    "latest_ckpt = max(ckpt_files, key=os.path.getmtime)\n",
    "print(\"Loading weights from:\", latest_ckpt)\n",
    "encoder.load_weights(latest_ckpt)\n",
    "\n",
    "# 2) Load a test image (first of your set)\n",
    "test_imgs = load_all_imgs(test_image_path)\n",
    "cover = test_imgs[0]\n",
    "\n",
    "# 3) Embed then extract\n",
    "stego     = encoder.predict([np.expand_dims(cover, 0), message_arr])[0]\n",
    "pred_bits = decoder.predict(np.expand_dims(stego, 0))[0]\n",
    "recovered = bits_to_text(pred_bits)\n",
    "\n",
    "# 4) Plot cover, stego, and recovered text\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Cover\")\n",
    "plt.imshow(cover)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Stego\")\n",
    "plt.imshow(stego)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(f\"Recovered:\\n{repr(recovered)}\")\n",
    "blank = np.ones_like(cover) * 0.8\n",
    "plt.imshow(blank)\n",
    "plt.text(0.5, 0.5, recovered, ha=\"center\", va=\"center\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477caf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Steganography Model: Combined Encoder-Decoder\n",
    "\n",
    "input_cover = keras.Input(shape=(128,128,3))\n",
    "input_msg = keras.Input(shape=(128,128,1))\n",
    "stego = encoder([input_cover, input_msg])\n",
    "decoded = decoder(stego)\n",
    "\n",
    "steganography_model = keras.Model([input_cover, input_msg], [stego, decoded])\n",
    "\n",
    "steganography_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=['mse', 'binary_crossentropy'],\n",
    "    loss_weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 5. Prepare Training Data\n",
    "\n",
    "def generate_batch(batch_size=8, image_files=None):\n",
    "    batch_images = []\n",
    "    batch_messages = []\n",
    "    for i in range(batch_size):\n",
    "        if image_files:\n",
    "            img = load_image(np.random.choice(image_files))\n",
    "        else:\n",
    "            img = np.random.rand(128,128,3)\n",
    "        msg = np.random.randint(0, 2, size=(128,128,1)).astype(np.float32)\n",
    "        batch_images.append(img)\n",
    "        batch_messages.append(msg)\n",
    "    return np.array(batch_images), np.array(batch_messages)\n",
    "\n",
    "# Update with your image paths:\n",
    "# image_folder = 'images'\n",
    "# os.makedirs(image_folder, exist_ok=True)\n",
    "image_folder = r'C:/Users/User/Desktop/stable_signature/data/train/train'\n",
    "# Place or generate some images in the folder before running!\n",
    "\n",
    "image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "if not image_files:\n",
    "    # Generate dummy images for demonstration\n",
    "    for i in range(4):\n",
    "        dummy = (np.random.rand(128,128,3)*255).astype(np.uint8)\n",
    "        Image.fromarray(dummy).save(f'{image_folder}/dummy_{i}.png')\n",
    "    image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "\n",
    "cover_batch, message_batch = generate_batch(8, image_files=image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train the Model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    cover_batch, message_batch = generate_batch(8, image_files=image_files)\n",
    "    loss = steganography_model.train_on_batch([cover_batch, message_batch], [cover_batch, message_batch])\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Demo: Embedding and Extracting a Text Key\n",
    "\n",
    "def create_folder(folder_name, base_path=None):\n",
    "    if base_path is None:\n",
    "        base_path = os.getcwd()\n",
    "    full_path = os.path.join(base_path, folder_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    return full_path\n",
    "\n",
    "def load_all_imgs(path, size=(128,128)):\n",
    "    imgs = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.lower().endswith(('.png','.jpg','.jpeg','.bmp','.tiff')):\n",
    "            full_path = os.path.join(path, filename)\n",
    "            img = Image.open(full_path).convert('RGB').resize(size)\n",
    "            imgs.append(np.asarray(img)/255.0)\n",
    "    return np.stack(imgs)\n",
    "\n",
    "# 3) Demo\n",
    "test_image_path = r\"C:/Users/User/Desktop/Watermarking_Project/stable_signature_2_0/test_images/\"\n",
    "all_imgs        = load_all_imgs(test_image_path)\n",
    "secret_message  = \"InvisibleKey!\"\n",
    "message_arr     = text_to_bits(secret_message)\n",
    "\n",
    "# create once, outside the loop\n",
    "results_dir = create_folder(\"stegnography_test_results\")\n",
    "\n",
    "for idx, cover_img in enumerate(all_imgs[:4]):\n",
    "    stego_img = encoder.predict([np.expand_dims(cover_img, 0), message_arr])[0]\n",
    "\n",
    "    # now dirs is a real string\n",
    "    out_path = os.path.join(results_dir, f'stego_image_{idx}.png')\n",
    "    save_image(stego_img, out_path)\n",
    "\n",
    "    # display\n",
    "    print(f\"Image #{idx} â€” Original vs Stego:\")\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for i, im in enumerate((cover_img, stego_img), 1):\n",
    "        plt.subplot(1,2,i)\n",
    "        plt.title('Original' if i==1 else 'Stego')\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"PSNR:\", psnr(cover_img, stego_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Extract the Message\n",
    "# stego_img_loaded = load_image('/stegnography_test_results/stego_image.png')\n",
    "all_test_imgs = load_all_imgs(results_dir)\n",
    "for idx, cover_img in enumerate(all_test_imgs[:4]):\n",
    "    pred_msg = decoder.predict(np.expand_dims(cover_img, 0))[0]\n",
    "    recovered_text = bits_to_text(pred_msg)\n",
    "    print(\"Recovered message:\", repr(recovered_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Batch Processing Example\n",
    "batch_secret = \"BatchKey\"\n",
    "for fname in os.listdir(test_image_path):\n",
    "    img_path = os.path.join(test_image_path, fname)\n",
    "    cover_img = load_image(img_path)\n",
    "    secret_msg = text_to_bits(batch_secret)\n",
    "    stego_img = encoder.predict([np.expand_dims(cover_img,0), secret_msg])[0]\n",
    "    save_image(stego_img, f'stego_{fname}')\n",
    "    # Extraction example\n",
    "    pred_msg = decoder.predict(np.expand_dims(stego_img,0))[0]\n",
    "    recovered = bits_to_text(pred_msg)\n",
    "    print(f\"{fname} â†’ Extracted: {repr(recovered[:len(batch_secret)])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save Models\n",
    "\n",
    "encoder.save('encoder_model.h5')\n",
    "decoder.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Demo: Embedding and Extracting a Text Key (with binary display)\n",
    "\n",
    "test_image_path = r\"C:/Users/User/Desktop/Watermarking_Project/stable_signature_2_0/test_images/\"\n",
    "secret_message = \"InvisibleKey!\"\n",
    "\n",
    "new_all_img = load_all_imgs(test_image_path)\n",
    "message_arr = text_to_bits(secret_message)\n",
    "stegno_dir = create_folder(\"stegnography_images\")\n",
    "for idx, cover_img in enumerate(new_all_img[:4]):\n",
    "    stego_img = encoder.predict([np.expand_dims(cover_img, 0), message_arr])[0]\n",
    "        # now dirs is a real string\n",
    "    out_path = os.path.join(stegno_dir, f'stego_{idx}.png')\n",
    "    save_image(stego_img, out_path)\n",
    "\n",
    "    # Display images\n",
    "    print(\"Original vs Stego Image:\")\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original')\n",
    "    plt.imshow(cover_img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Stego')\n",
    "    plt.imshow(stego_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"PSNR:\", psnr(cover_img, stego_img))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the message as bits\n",
    "# Load your stego images\n",
    "stego_all_img = load_all_imgs(stegno_dir)\n",
    "\n",
    "# Prepare the input key in binary once\n",
    "input_bin = ''.join(format(ord(c), '08b') for c in secret_message)\n",
    "\n",
    "# Function to turn a decoded bit-array into an 0/1 string\n",
    "def bits_to_binstr(bit_array, bit_len):\n",
    "    flat = bit_array.flatten()\n",
    "    # threshold at 0.5\n",
    "    return ''.join(str(int(b > 0.5)) for b in flat[:bit_len])\n",
    "\n",
    "# Process exactly 4 images (or fewer if you have <4)\n",
    "for idx, image in enumerate(stego_all_img[:4]):\n",
    "    # 1) Decode\n",
    "    pred_msg = decoder.predict(np.expand_dims(image, 0))[0]\n",
    "    \n",
    "    # 2) Convert to binary string\n",
    "    extracted_bin = bits_to_binstr(pred_msg, len(secret_message) * 8)\n",
    "    \n",
    "    # 3) Recover text\n",
    "    recovered_text = bits_to_text(pred_msg)\n",
    "    if recovered_text == secret_message:\n",
    "        print(f\"Image #{idx}: Perfect recovery âœ…\")\n",
    "    else:\n",
    "        print(f\"Image #{idx}: MISMATCH ðŸš¨\")\n",
    "        print(\"  Expected:\", repr(secret_message))\n",
    "        print(\"  Got:     \", repr(recovered_text))\n",
    "# 4) Print results\n",
    "print(f\"Image #{idx}\")\n",
    "print(f\"  Input (bin):     {input_bin}\")\n",
    "print(f\"  Extracted (bin): {extracted_bin}\")\n",
    "print(f\"  Recovered text:  {repr(recovered_text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa155dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bits     = [int(b) for b in input_bin]\n",
    "extracted_bits = [int(b) for b in extracted_bin]\n",
    "errors = sum(i!=j for i,j in zip(input_bits, extracted_bits))\n",
    "ber = errors / len(input_bits)\n",
    "print(f\"  Bit errors: {errors}/{len(input_bits)} ({ber:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Prepare test image and secret\n",
    "cover_img = load_image(test_image_path)\n",
    "secret_message = \"InvisibleKey!\"\n",
    "message_arr = text_to_bits(secret_message)\n",
    "\n",
    "# Test the model: Encrypt (Alice)\n",
    "start_time = time.time()\n",
    "stego_img = encoder.predict([np.expand_dims(cover_img, 0), message_arr])[0]\n",
    "alice_encrypt_time = time.time() - start_time\n",
    "\n",
    "# Test the model: Decrypt (Bob)\n",
    "start_time = time.time()\n",
    "bob_output = decoder.predict(np.expand_dims(stego_img, 0))[0]\n",
    "bob_decrypt_time = time.time() - start_time\n",
    "\n",
    "# Loss/accuracy computation (optional)\n",
    "bob_loss_test = np.mean(np.abs(message_arr[0] - bob_output))\n",
    "\n",
    "print('Alice encrypt time:', alice_encrypt_time)\n",
    "print('Bob decrypt time:', bob_decrypt_time)\n",
    "print('Bob mean absolute error (loss):', bob_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bae872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob's predicted output (binary array)\n",
    "b1 = np.round(bob_output, 1)\n",
    "b1 = b1.ravel()\n",
    "b1 = np.abs(b1)\n",
    "print('bob_output == ', b1)\n",
    "\n",
    "# Convert to binary string\n",
    "b2 = ''.join(str(int(bit)) for bit in b1[:len(secret_message)*8])\n",
    "print('b2 (binary) =', b2)\n",
    "\n",
    "# Convert binary string to ASCII\n",
    "n = int(b2, 2)\n",
    "try:\n",
    "    str2 = n.to_bytes((n.bit_length() + 7) // 8, 'big').decode(errors='ignore')\n",
    "except Exception as e:\n",
    "    str2 = \"Decode error: \" + str(e)\n",
    "print(\"Bob recovered plain text:\", repr(str2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have an 'eve' model:\n",
    "# eve_output = eve.predict(np.expand_dims(stego_img, 0))[0]\n",
    "# eve_loss_test = np.mean(np.abs(message_arr[0] - eve_output))\n",
    "# print('Eve mean absolute error (loss):', eve_loss_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DFTF",
   "language": "python",
   "name": "dftf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
